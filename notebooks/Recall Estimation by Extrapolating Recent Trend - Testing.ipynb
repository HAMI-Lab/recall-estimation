{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a3c2c3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy.stats import poisson, nbinom\n",
    "from scipy.optimize import curve_fit\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "from pypdf import PdfMerger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b79424f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate recall for a given data frame (data) and parameters (param)\n",
    "# data:\n",
    "#    data is a pandas DataFrame, sorted by scores from high to low\n",
    "#    data has a column 'Score': classifier-predicted score\n",
    "#    data has a column 'Label': 1: relevant; 0: non-relevant; -1: unlabeled \n",
    "# param:\n",
    "#    param is a dictionary\n",
    "\n",
    "#        'ignore_score': bool\n",
    "#        if not 'ignore_score':\n",
    "#            'score_threshold': double [0.0, 1.0]\n",
    "#        'interval_length': int, >= 1\n",
    "#        'rate_curve': {'exp', 'poly', 'exp_poly', 'const'}\n",
    "#        'lookback': int, >= 1\n",
    "#        'forget_factor': double, >= 0\n",
    "#        'est_num_rel': {'all', 'rest'}\n",
    "#        'confidence_bound': {'none', 'poisson'}\n",
    "#        'confidence_level': double [0.0, 1.0]\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4db0d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processes the 'Label' and 'Score' columns of the input data\n",
    "# such that each reference has a guessed/pseudo inclusion/exclusion label\n",
    "# 1 means inclusion, 0 means exclusion\n",
    "# if param['ignore_score'] is True, \n",
    "#     then we assign all unlabeled data a pseudo exclusion label\n",
    "# if param['ignore_score'] is False,\n",
    "#     then we assign an unlabeled data point a pseudo inclusion label \n",
    "#     if it has a predicted score no less than param['score_threshold']\n",
    "#     otherwise, we assign an unlabeled data point a pseudo exclusion label\n",
    "# \n",
    "# params:\n",
    "#     data: input pandas DataFrame\n",
    "#     param: global parameter dictionary\n",
    "# return:\n",
    "#     a list of pseudo labels\n",
    "#     1: inclusion; 0: exclusion\n",
    "\n",
    "def get_inclusion_list(data, param):\n",
    "    l = []\n",
    "    if param['ignore_score']:\n",
    "        for i, row in data.iterrows():\n",
    "            if row['Label'] < 0:\n",
    "                l.append(0)\n",
    "            else:\n",
    "                l.append(row['Label'])\n",
    "    else:\n",
    "        for i, row in data.iterrows():\n",
    "            if row['Label'] < 0:\n",
    "                if row['Score'] >= param['score_threshold']:\n",
    "                    l.append(1)\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            else:\n",
    "                l.append(row['Label'])\n",
    "    return l    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17d471d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exponential rate function \n",
    "# x: position index, starting from 1\n",
    "# a: function parameter, to be determined by curve-fitting\n",
    "# a should be greater than 0. large a means fast rate decay \n",
    "# rate = exp^{-a * (position - 1)}\n",
    "\n",
    "def exponential_rate(x, a):\n",
    "    return np.exp(-a * (x - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "966b562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial rate function \n",
    "# x: position index, starting from 1\n",
    "# a: function parameter, to be determined by curve-fitting\n",
    "# a should be greater than 0. large a means fast rate decay \n",
    "# rate = position^{-a}\n",
    "\n",
    "def polynomial_rate(x, a):\n",
    "    return np.power(x, -a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "976dad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixture of exponential and polynomial rate function \n",
    "# x: position index, starting from 1\n",
    "# a1: exponential function parameter, to be determined by curve-fitting\n",
    "# a1: polynomial function parameter, to be determined by curve-fitting\n",
    "# c: weight of exponential. (1-c): weight of polynomial\n",
    "# a1, a2 should be greater than 0. large a1, a2 means fast rate decay\n",
    "# c should be between 0 and 1.\n",
    "# rate = c * exp^{-a * (position - 1)}  +  (1-c) * position^{-a}\n",
    "\n",
    "def exp_poly_rate(x, a1, a2, c):\n",
    "    return c * np.exp(-a1 * (x - 1)) + (1-c) * np.power(x, -a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13dab7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant rate function \n",
    "# x: position index, starting from 1\n",
    "# a: function parameter, to be determined by curve-fitting\n",
    "# a should be greater than 0. larger a means high inclusion rate \n",
    "# rate = a, for all positions\n",
    "\n",
    "def constant_rate(x, a):\n",
    "    return a * np.ones_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc6327a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function used in fit_inclusion_rate_ls\n",
    "# travel back from the end of y's, until we see param['lookback'] \n",
    "# inclusions. return the travel length\n",
    "\n",
    "def get_lookback_travel_length(y_list, lookback):\n",
    "    num_rel_so_far = 0\n",
    "    # traverse the y_list in reverse order\n",
    "    for i in range(1, len(y_list) + 1):\n",
    "        if y_list[-i] > 0:\n",
    "            num_rel_so_far += 1\n",
    "        if num_rel_so_far == lookback:\n",
    "            return i\n",
    "    return len(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1a8de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function used in fit_inclusion_rate_ls\n",
    "# smooth x's and y's by averaging y's in equal-length intervals\n",
    "# smoothing helps mitigate overfitting when the rate function\n",
    "# is not constant.\n",
    "# smoothing is not needed when the rate function is a constant.\n",
    "\n",
    "def smooth(x, y, param):\n",
    "    interval_length = param['interval_length']\n",
    "    if len(x) <= interval_length:\n",
    "        return [np.average(x)], [np.average(y)]\n",
    "    \n",
    "    new_x, new_y = [], []\n",
    "    num_intervals = int(len(x) / interval_length)\n",
    "    interval_multiples = num_intervals * interval_length\n",
    "    start_pos = 0\n",
    "    \n",
    "    for i in range(num_intervals):\n",
    "        new_x.append( sum(x[start_pos : start_pos+interval_length])/interval_length )\n",
    "        new_y.append( sum(y[start_pos : start_pos+interval_length])/interval_length )\n",
    "        start_pos += interval_length\n",
    "    \n",
    "    if len(x) > interval_multiples:\n",
    "        new_x.append( sum(x[interval_multiples:]) / (len(x) - interval_multiples) )\n",
    "        new_y.append( sum(y[interval_multiples:]) / (len(x) - interval_multiples) )\n",
    "    \n",
    "    return new_x, new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9172a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit inclusion rate using nonlinear least square curve-fitting\n",
    "# params:\n",
    "#    inc_list: inclusion list, same length as the input dataframe\n",
    "#              each entry is a pseudo inclusion label (1 or 0)\n",
    "#    curr_pos: current position, an index in [0, len(inc_list)-1]\n",
    "#              the current screening position. we will only use\n",
    "#              the inc_list before curr_pos to fit a rate function\n",
    "#    param:    global parameter dictionary\n",
    "# return:\n",
    "#    rate: a list with equal length as inc_list\n",
    "#              each entry is an estimated probability of inclusion\n",
    "\n",
    "def fit_inclusion_rate_ls(inc_list, curr_pos, param):\n",
    "    # prepare the list of x's and y's for curve fitting\n",
    "    x, y = [], []\n",
    "    \n",
    "    # first, take out pseudo labels from inc_list up to curr_pos\n",
    "    # for example:\n",
    "    # inc_list:     1 0 1 1 0 ... 0   1   0            0   0   ...   0\n",
    "    # position:     1 2 3 4 5 ... 118 119 curr_pos=120 121 122 ...   end\n",
    "    #               |<------- considered ----------->|\n",
    "    for i in range(curr_pos):\n",
    "        x.append( i + 1 )\n",
    "        y.append( inc_list[i]  )\n",
    "    \n",
    "    # if no relevant articles from beginning to curr_pos\n",
    "    # let's don't give up so quickly. assume there were\n",
    "    # one relevant document at the beginning\n",
    "    #\n",
    "    # TODO: add heuristic method to detect if:\n",
    "    #       1) the inclusion rate is too low \n",
    "    #       2) the ranker seems to perform worse than random\n",
    "    # if so, switch to conservative rate estimates\n",
    "    #\n",
    "    if sum(y) == 0:\n",
    "        y[0] = 1\n",
    "    \n",
    "    # print ('full_x', x)\n",
    "    # print ('full_y', y)\n",
    "    \n",
    "    # travel back from the end of y's, until we see param['lookback'] \n",
    "    # inclusions. return the travel length\n",
    "    travel_length = get_lookback_travel_length(y, param['lookback'])\n",
    "    \n",
    "    # only take the most recent x's and y's within travel_length\n",
    "    # for example: travel_length = 117\n",
    "    # inc_list:     1 0 1 1 0 ... 0   1   0            0   0   ...   0\n",
    "    # position:     1 2 3 4 5 ... 118 119 curr_pos=120 121 122 ...   end\n",
    "    #                     |<---- considered -------->| \n",
    "    x, y = x[-travel_length:], y[-travel_length:]\n",
    "    \n",
    "    # print ('recent_x', x)\n",
    "    # print ('recent_y', y)\n",
    "    \n",
    "    # smooth x's and y's by averaging y's in equal-length intervals\n",
    "    # smoothing helps mitigate overfitting when the rate function\n",
    "    # is not constant.\n",
    "    # smoothing is not needed when the rate function is a constant.\n",
    "    x, y = smooth(x, y, param)\n",
    "    \n",
    "    # print ('smooth_x', x)\n",
    "    # print ('smooth_y', y)\n",
    "    \n",
    "    if param['rate_curve'] == 'exp':\n",
    "        func = exponential_rate\n",
    "    elif param['rate_curve'] == 'poly':\n",
    "        func = polynomial_rate\n",
    "    elif param['rate_curve'] == 'const':\n",
    "        func = constant_rate\n",
    "    elif param['rate_curve'] == 'exp_poly':\n",
    "        func = exp_poly_rate\n",
    "    \n",
    "    # the i-th entry in sigma_list indicates the importance \n",
    "    # of the i-th (x,y) data point in curve fitting\n",
    "    # the larger sigma, the less important a data point\n",
    "    # we set the sigma to be larger for data points that are\n",
    "    # 'further back in the past', i.e., to the left of the x's and y's\n",
    "    # this achieves a forgetting effect: \n",
    "    # the further back an inclusion event, the less influence it has \n",
    "    # on estimating the rate function \n",
    "    \n",
    "    # here, we implement the sigma_list as an increasing function\n",
    "    # with respect to the reversed position of a data point, so:\n",
    "    #    the most recent data point has sigma = 1 \n",
    "    #    the second most recent data point has sigma slightly larger than 1\n",
    "    #    and so on.\n",
    "    #\n",
    "    # param['forget_factor'] controlls how fast the forgetting is.\n",
    "    # param['forget_factor'] = 0: no forgetting\n",
    "    # param['forget_factor'] > 0: the larger the value, the faster the forgetting.\n",
    "    \n",
    "    # sigma_list = 1 + param['forget_factor'] / 1000 * np.power(np.array(range(1, len(x)+1)), 2)\n",
    "    sigma_list = np.exp(param['forget_factor'] / 50 * np.array(range(len(x))))\n",
    "    sigma_list = np.flip(sigma_list)\n",
    "    # print ('sigma_list', sigma_list)\n",
    "    \n",
    "    # fit the rate function using the most recent x's and y's with forgetting\n",
    "    if param['rate_curve'] == 'exp' or param['rate_curve'] == 'poly' or param['rate_curve'] == 'const': \n",
    "        popt, pcov = curve_fit(func, x, y, sigma = sigma_list, maxfev = 500)\n",
    "    elif param['rate_curve'] == 'exp_poly':\n",
    "        popt, pcov = curve_fit(func, x, y, sigma = sigma_list, maxfev = 500, bounds=(0, [np.inf, np.inf, 1.0]))\n",
    "    \n",
    "    # use the fitted curve to generate the rates at each position \n",
    "    x_array = np.array(range(1, len(inc_list)+1))\n",
    "    rate = func(x_array, *popt)\n",
    "    \n",
    "    # print ('popt', *popt)\n",
    "    # print (rate)\n",
    "\n",
    "    \n",
    "    # plt.plot(x, y, 'b-', label='data for fitting')\n",
    "    # if param['rate_curve'] == 'exp' or param['rate_curve'] == 'poly' or param['rate_curve'] == 'const':\n",
    "    #    plt.plot(x_array, rate, 'r-', label='fit: a=%5.3f' % tuple(popt))\n",
    "    # else:\n",
    "    #    plt.plot(x_array, rate, 'r-', label='fit: a1=%5.3f, a2=%5.3f, c=%5.3f' % tuple(popt))\n",
    "    # plt.xlabel('rank position')\n",
    "    # plt.ylabel('inclusion rate')\n",
    "    # plt.ylim([0, 1])\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "    \n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b924e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall is a ratio:\n",
    "#               number of included documents so far\n",
    "#               ----------------------------------------- \n",
    "#               total number of documents to be included \n",
    "# we know the denominator, by counting included documents so far\n",
    "# we can estimate the denominator in two ways:\n",
    "# 1) param['est_num_rel'] == 'all':\n",
    "#    total number of documents estimated to be included\n",
    "# 2) param['est_num_rel'] == 'rest':\n",
    "#    number of included documents so far \n",
    "#       + number of *unlabeled* documents estimated to be included\n",
    "\n",
    "def take_ratio(data, rate, param):\n",
    "    num_rel = len(data[data['Label'] == 1]) + 1e-20\n",
    "    if param['est_num_rel'] == 'all':\n",
    "        est_num_rel_all = sum(rate)\n",
    "        if param['confidence_bound'] == 'none':\n",
    "            denominator = est_num_rel_all\n",
    "        elif param['confidence_bound'] == 'poisson':\n",
    "            denominator = poisson.ppf(param['confidence_level'], est_num_rel_all)\n",
    "        else: # param['confidence_bound'] == 'none'\n",
    "            denominator = est_num_rel_all\n",
    "        return num_rel / denominator\n",
    "    else: # param['est_num_rel'] == 'rest':\n",
    "        est_num_rel_rest = sum([rate[i] for i, row in data.iterrows() if row['Label'] == -1])\n",
    "        # print ('est_num_rel_rest', est_num_rel_rest)\n",
    "        if param['confidence_bound'] == 'none':\n",
    "            partial_denominator = est_num_rel_rest\n",
    "        elif param['confidence_bound'] == 'poisson':\n",
    "            partial_denominator = poisson.ppf(param['confidence_level'], est_num_rel_rest)\n",
    "        else: # param['confidence_bound'] == 'none'\n",
    "            partial_denominator = est_num_rel_rest\n",
    "        return num_rel / (num_rel + partial_denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5075f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_recall(data, current_position, param):\n",
    "    \n",
    "    # inclusion_list is a Python list that has equal length as data\n",
    "    # it processes the 'Label' and 'Score' columns of the input data\n",
    "    # such that each reference has a *guessed* inclusion/exclusion label\n",
    "    # 1 means inclusion, 0 means exclusion\n",
    "    inclusion_list = get_inclusion_list(data, param)\n",
    "    \n",
    "    # rate is a Python list that has equal length as data\n",
    "    # each value in the list is an estimated rate \n",
    "    # (i.e., probability) of inclusion\n",
    "    rate = fit_inclusion_rate_ls(inclusion_list, current_position, param)\n",
    "    \n",
    "    # calculate recall using the estimated rate from above\n",
    "    recall_est = take_ratio(data, rate, param)\n",
    "    \n",
    "    return recall_est"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df34fd4",
   "metadata": {},
   "source": [
    "# Testing Recall Estimation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54bf9f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a data table for plotting:\n",
    "# cut_off, num_rel, true_recall, [estimation methods]\n",
    "\n",
    "def generate_plot_data(all_data, param_dict, step_size, data_path):\n",
    "    out_f = open(data_path, 'w')\n",
    "    out_f.write('\\t'.join(['cut_off', 'num_rel', 'true_recall']))\n",
    "    for method_name, param in params.items():\n",
    "        out_f.write('\\t' + method_name)\n",
    "    out_f.write('\\n')\n",
    "        \n",
    "    num_rel = 0\n",
    "    total_rel = len(all_data[all_data['Label'] == 1])\n",
    "    for i, row in all_data.iterrows():\n",
    "        if row['Label'].item() == 1:\n",
    "            num_rel += 1\n",
    "        if i % step_size == 0 and i > 0:\n",
    "            cutoff = i\n",
    "            all_data_copy = all_data.copy()\n",
    "            for j, row in all_data_copy.iterrows():\n",
    "                if j > cutoff:\n",
    "                    all_data_copy.at[j,'Label'] = -1\n",
    "            \n",
    "            true_r = num_rel / total_rel\n",
    "            est_r_list = []\n",
    "            for method_name, param in params.items():\n",
    "                est_r = estimate_recall(all_data_copy, cutoff, param)\n",
    "                est_r_list.append(est_r)\n",
    "                \n",
    "            out_f.write('\\t'.join([str(i), str(num_rel), str(true_r)]))\n",
    "            for est_r in est_r_list:\n",
    "                out_f.write('\\t' + str(est_r))\n",
    "            out_f.write('\\n')\n",
    "            \n",
    "            if min(est_r_list) > 0.99:\n",
    "                break\n",
    "    out_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "94150606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the oscillating trajectory of recall estimation curves\n",
    "# assuming that the plot data has been generated using generate_plot_data\n",
    "\n",
    "def plot_true_and_estimated_recall_curves(all_data, plot_data, params, source, topic, num_screened, plot_path):\n",
    "    total_rel = len(all_data[all_data['Label'] == 1])\n",
    "    plot_data_df = pd.read_csv(plot_data, sep = '\\t')\n",
    "    \n",
    "    cut_off = plot_data_df['cut_off'].tolist()\n",
    "    true_recall = plot_data_df['true_recall'].tolist()\n",
    "    \n",
    "    plt.title('{}, {}, rel: {}, screened: {}, S: {}'.format(source, topic, total_rel, num_screened, len(all_data)))\n",
    "    plt.plot(cut_off, true_recall, 'r', label='true recall')\n",
    "    for method_name, param in params.items():\n",
    "        plt.plot(cut_off, plot_data_df[method_name].tolist(), LINE_COLOR[method_name], label = method_name, linewidth=0.75)\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.axhline(y = 0.95, color = 'magenta', linestyle = '--')\n",
    "\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.xlabel('Rank position')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a91ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot one snapshot of recall estimation curve\n",
    "# subroutine called in plot_recall_curves_snapshots\n",
    "\n",
    "def plot_one_snapshot(rel_pos, cutoff, row_idx, true_recall, esti_recall, data_size, plot_path):\n",
    "    rel_pos_i = [rp for rp in rel_pos if rp <= cutoff]\n",
    "    num_rel = len(rel_pos_i)\n",
    "    total_rel = len(rel_pos)\n",
    "    plt.title('{}, {}, rel: {}, screened: {}, S: {}'.format(source, topic, total_rel, num_screened, data_size))\n",
    "    plt.plot(rel_pos_i + [cutoff], [(k+1)/total_rel for k in range(num_rel)] + [true_recall[row_idx]], 'r', label = 'true recall')\n",
    "    for method_name, param in params.items():\n",
    "        est_r = esti_recall[method_name][row_idx]\n",
    "        est_total_rel = num_rel / est_r\n",
    "        plt.plot(rel_pos_i + [cutoff], [(k+1)/est_total_rel for k in range(num_rel)] + [est_r], LINE_COLOR[method_name], label = method_name, linewidth=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.axhline(y = 0.95, color = 'magenta', linestyle = '--')\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.xlabel('Rank position')\n",
    "    plt.xlim(0, data_size)\n",
    "    plt.ylabel('Recall')\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7cd0bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot snapshots of recall estimation curve over time\n",
    "# assuming that the plot data has been generated using generate_plot_data\n",
    "\n",
    "def plot_recall_curves_snapshots(all_data, plot_data, params, source, topic, num_screened, num_snapshots, plot_path_prefix):\n",
    "\n",
    "    total_rel = len(all_data[all_data['Label'] == 1])\n",
    "    rel_pos = []\n",
    "    for i, row in all_data.iterrows():\n",
    "        if row['Label'].item() == 1:\n",
    "            rel_pos.append(i + 1)\n",
    "            \n",
    "    plot_data_df = pd.read_csv(plot_data, sep = '\\t')\n",
    "    \n",
    "    cut_off = plot_data_df['cut_off'].tolist()\n",
    "    true_recall = plot_data_df['true_recall'].tolist()\n",
    "    esti_recall = {}\n",
    "    for method_name, param in params.items():\n",
    "        esti_recall[method_name] = plot_data_df[method_name].tolist()\n",
    "        \n",
    "    step_size = int(np.round(len(cut_off) / num_snapshots))\n",
    "    \n",
    "    for j in range(num_snapshots - 1):\n",
    "        row_idx = j  * step_size\n",
    "        i = cut_off[row_idx]\n",
    "        plot_path = plot_path_prefix + '_{}.pdf'.format(i)\n",
    "        plot_one_snapshot(rel_pos, i, row_idx, true_recall, esti_recall, len(all_data), plot_path)\n",
    "        \n",
    "    row_idx = len(cut_off) - 1\n",
    "    i = cut_off[-1]\n",
    "    plot_path = plot_path_prefix + '_{}.pdf'.format(i)\n",
    "    plot_one_snapshot(rel_pos, i, row_idx, true_recall, esti_recall, len(all_data), plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0660e0",
   "metadata": {},
   "source": [
    "# Baseline: SWIFT; Forget Factors: 0.1, 0.2, 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a789543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_forget_01 = {'ignore_score': True,\n",
    "         'interval_length': 2,\n",
    "         'lookback': 5,\n",
    "         'forget_factor': 0.1,\n",
    "         'rate_curve': 'const',\n",
    "         'est_num_rel': 'rest',\n",
    "         'confidence_bound': 'poisson',\n",
    "         'confidence_level': 0.95\n",
    "        }\n",
    "\n",
    "param_forget_02 = {'ignore_score': True,\n",
    "         'interval_length': 2,\n",
    "         'lookback': 5,\n",
    "         'forget_factor': 0.2,\n",
    "         'rate_curve': 'const',\n",
    "         'est_num_rel': 'rest',\n",
    "         'confidence_bound': 'poisson',\n",
    "         'confidence_level': 0.95\n",
    "        }\n",
    "\n",
    "param_forget_05 = {'ignore_score': True,\n",
    "         'interval_length': 2,\n",
    "         'lookback': 5,\n",
    "         'forget_factor': 0.5,\n",
    "         'rate_curve': 'const',\n",
    "         'est_num_rel': 'rest',\n",
    "         'confidence_bound': 'poisson',\n",
    "         'confidence_level': 0.95\n",
    "        }\n",
    "\n",
    "param_swift = {'ignore_score': True,\n",
    "         'interval_length': 2,\n",
    "         'lookback': 5,\n",
    "         'forget_factor': 0,\n",
    "         'rate_curve': 'const',\n",
    "         'est_num_rel': 'rest',\n",
    "         'confidence_bound': 'none',\n",
    "         'confidence_level': 0.95\n",
    "        }\n",
    "\n",
    "params = {'SWIFT': param_swift, 'SWIFT+forget0.1': param_forget_01, 'SWIFT+forget0.2': param_forget_02, 'SWIFT+forget0.5': param_forget_05}\n",
    "\n",
    "LINE_COLOR = {'SWIFT': 'tab:green', 'SWIFT+forget0.1': 'tab:cyan', 'SWIFT+forget0.2': 'tab:blue', 'SWIFT+forget0.5': 'tab:orange'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634aa660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate plot data by running recall estimation methods on datasets\n",
    "# this step could take a few hours \n",
    "\n",
    "per_topic_data_folder = '../data/per_topic_simulation'\n",
    "per_topic_output_folder = '../output/per_topic_results'\n",
    "\n",
    "for file in os.listdir(per_topic_data_folder):\n",
    "    file_path = os.path.join(per_topic_data_folder, file)\n",
    "    source, topic, num_screened = file.split('.')[0].split('_')\n",
    "    num_screened = int(num_screened)\n",
    "    rank_dataframe = pd.read_csv(file_path)\n",
    "    data_path = os.path.join(per_topic_output_folder, file.replace('.csv', '.txt'))\n",
    "    \n",
    "    print ('{}, {}, screened: {}'.format(source, topic, num_screened))\n",
    "    \n",
    "    generate_plot_data(rank_dataframe, params, 30, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc7f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot recall trajectories on datasets \n",
    "# this step is fast as we have generated the plot data\n",
    "\n",
    "per_topic_data_folder = '../data/per_topic_simulation'\n",
    "per_topic_output_folder = '../output/per_topic_results.2'\n",
    "\n",
    "for file in os.listdir(per_topic_data_folder):\n",
    "\n",
    "    file_path = os.path.join(per_topic_data_folder, file)\n",
    "    source, topic, num_screened = file.split('.')[0].split('_')\n",
    "    num_screened = int(num_screened)\n",
    "    rank_dataframe = pd.read_csv(file_path)\n",
    "    data_path = os.path.join(per_topic_output_folder, file.replace('.csv', '.txt'))\n",
    "    \n",
    "    print ('{}, {}, screened: {}'.format(source, topic, num_screened))\n",
    "    \n",
    "    plot_path = os.path.join(per_topic_output_folder, file.replace('.csv', '_recall_trajectory.pdf'))\n",
    "    plot_true_and_estimated_recall_curves(rank_dataframe, data_path, params, source, topic, num_screened, plot_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f778eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot recall curve snapshots on datasets \n",
    "# this step is fast as we have generated the plot data\n",
    "\n",
    "per_topic_data_folder = '../data/per_topic_simulation'\n",
    "per_topic_output_folder = '../output/per_topic_results'\n",
    "\n",
    "num_snapshots = 10\n",
    "\n",
    "for file in os.listdir(per_topic_data_folder):\n",
    "    file_path = os.path.join(per_topic_data_folder, file)\n",
    "    source, topic, num_screened = file.split('.')[0].split('_')\n",
    "    num_screened = int(num_screened)\n",
    "    rank_dataframe = pd.read_csv(file_path)\n",
    "    data_path = os.path.join(per_topic_output_folder, file.replace('.csv', '.txt'))\n",
    "    \n",
    "    print ('{}, {}, screened: {}'.format(source, topic, num_screened))\n",
    "    \n",
    "    plot_path_prefix = os.path.join(per_topic_output_folder, file.replace('.csv', '_snapshot'))\n",
    "    plot_recall_curves_snapshots(rank_dataframe, data_path, params, source, topic, num_screened, num_snapshots, plot_path_prefix)\n",
    "    snapshot_files = [os.path.join(per_topic_output_folder, f) for f in os.listdir(per_topic_output_folder) if file.replace('.csv', '_snapshot_') in f]\n",
    "    \n",
    "    merger = PdfMerger()\n",
    "    for pdf in sorted(snapshot_files, key = lambda x: int(x.split('_')[-1].split('.')[0])):\n",
    "        merger.append(pdf)\n",
    "    \n",
    "    plot_path = os.path.join(per_topic_output_folder, file.replace('.csv', '_recall_curve_snapshots.pdf'))\n",
    "    merger.write(plot_path)\n",
    "    merger.close()\n",
    "    \n",
    "    for f in snapshot_files:\n",
    "        os.remove(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
